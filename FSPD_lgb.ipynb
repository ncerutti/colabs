{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers torch wandb tqdm lightgbm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport lightgbm as lgb\nimport numpy as np\nimport os\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.utils.class_weight import compute_sample_weight\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import OneCycleLR\nfrom torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\nfrom tqdm.notebook import tqdm\nimport wandb\n\nprint(torch.__version__)\n\nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\n    print(\"All good\")\n    torch.cuda.empty_cache()\nelse:\n    device = torch.device(\"cpu\")\n    print(\"No GPU!!!\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wandb.login()\nwandb.init(project=\"FSPD\", config={\"architecture\": \"LightGBM\", \"epochs\": 100, \"batch_size\": \"NA\", \"learning_rate\": 0.05, \"hidden_size\": NA})","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"checkpoint_dir = \"kaggle/ModelCheckpoints\"\nos.makedirs(checkpoint_dir, exist_ok=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create clean_fspd function. This function will take in the fspd dataframe and return a cleaned version of it.\n\ndef clean_fspd(fspd_f):\n    \"\"\"This function takes in the fspd dataframe and returns a cleaned version of it.\n    \"\"\"\n    # Create a list of columns that are not needed\n    drop_cols = [\"lever\", \"itype\", \"source1link\", \"framework\", \"iso\", \"region_wb\", \"income_group2\", \"defn\", \"initialdate\", \"inclusion\", \"envitarget\", \"diethealth\"]\n    # Drop the columns in drop_cols from fspd_f\n    fspd_f = fspd_f.drop(columns=drop_cols)\n\n    # Replace the values in \"covid_mentioned\" with 0 if they are \"nan\"\n    fspd_f[\"covid_mentioned\"] = fspd_f[\"covid_mentioned\"].replace(np.nan, 0)\n\n    # Replace the values in \"targeted\" with 0 if the are \"\" o \"N\" and with 1 if they are \"Y\"\n    fspd_f[\"targeted\"] = fspd_f[\"targeted\"].replace(\"\", 0)\n    fspd_f[\"targeted\"] = fspd_f[\"targeted\"].replace(\"N\", 0)\n    fspd_f[\"targeted\"] = fspd_f[\"targeted\"].replace(\"Y\", 1)\n\n    # replace \"policy_code\" with 0 if it is empty\n    fspd_f[\"policy_code\"] = fspd_f[\"policy_code\"].replace(np.nan, 0)\n    fspd_f[\"y_end\"] = fspd_f[\"y_end\"].replace(np.nan, 0)\n    fspd_f[\"y_start\"] = fspd_f[\"y_start\"].replace(\"\", 0)\n    \n    return fspd_f\n\n\n\ndef encode_fspd(fspd_f):\n    \"\"\"This function takes in the fspd dataframe and returns the dataframe with one-hot encoding of a list of variables.\n    \"\"\"\n    to_encode = [\"country\", \"db\", \"policy_code\", \"y_start\", \"y_end\", \"income_group\", \"fsd_group\"]\n    fspd_f = pd.get_dummies(fspd_f, columns=to_encode)\n    return fspd_f\n\n\ndef get_non_text_features(batch_data, non_text_features):\n    batch_indices = batch_data[\"index\"].numpy()\n    batch_non_text_features = non_text_features.loc[batch_indices]\n    batch_non_text_features_tensor = torch.tensor(batch_non_text_features.values, dtype=torch.float32)\n    return batch_non_text_features_tensor\n\n\ndef compute_class_weights(y):\n    unique_classes = np.unique(y)\n    class_weights = compute_class_weight('balanced', classes=unique_classes, y=y)\n    return dict(zip(unique_classes, class_weights))\n\n\ndef get_sample_weights(y, class_weights):\n    return np.array([class_weights[cls] for cls in y])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FSPData(Dataset):\n    def __init__(self, data, target_segment):\n        self.data = data\n        #self.target_lever = target_lever\n        self.target_segment = target_segment\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        item = self.data.iloc[idx]\n        return {\n            \"index\": torch.tensor(idx, dtype=torch.long),  # Add this line\n            \"policydecision_details\": item[\"policydecision_details_tokens\"],\n            \"policy_description\": item[\"policy_description_tokens\"],\n            \"contextoradditionalinfo\": item[\"contextoradditionalinfo_tokens\"],\n            \"source1name\": item[\"source1name_tokens\"],\n            # Include other features as needed\n            #\"lever\": torch.tensor(self.target_lever[idx], dtype=torch.long),\n            \"segment\": torch.tensor(self.target_segment[idx], dtype=torch.long)\n        }\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def extract_features(data_loader):\n    features = []\n    labels = []\n\n    with torch.no_grad():\n        for batch_idx, batch_data in enumerate(tqdm(data_loader, desc=\"Extracting features\")):\n            # Move tensors to the device\n            batch_data[\"policydecision_details\"] = batch_data[\"policydecision_details\"].to(device)\n            batch_data[\"policy_description\"] = batch_data[\"policy_description\"].to(device)\n            batch_data[\"contextoradditionalinfo\"] = batch_data[\"contextoradditionalinfo\"].to(device)\n            batch_data[\"source1name\"] = batch_data[\"source1name\"].to(device)\n\n            # Obtain embeddings for each text feature\n            policydecision_details_embeddings = bert_model(batch_data[\"policydecision_details\"])\n            policy_description_embeddings = bert_model(batch_data[\"policy_description\"])\n            contextoradditionalinfo_embeddings = bert_model(batch_data[\"contextoradditionalinfo\"])\n            source1name_embeddings = bert_model(batch_data[\"source1name\"])\n\n            # Concatenate embeddings\n            combined_embeddings = torch.cat((policydecision_details_embeddings.last_hidden_state[:, 0, :],\n                                            policy_description_embeddings.last_hidden_state[:, 0, :],\n                                            contextoradditionalinfo_embeddings.last_hidden_state[:, 0, :],\n                                            source1name_embeddings.last_hidden_state[:, 0, :]), dim=1)\n            \n            # Concatenate non-text features\n            batch_non_text_features = get_non_text_features(batch_data, non_text_features)\n            batch_non_text_features = batch_non_text_features.to(device)\n            combined_features = torch.cat((combined_embeddings, batch_non_text_features), dim=1)\n\n            features.append(combined_features.cpu().numpy())\n            labels.append(batch_data[\"segment\"].cpu().numpy())\n\n    features = np.vstack(features)\n    labels = np.hstack(labels)\n    return features, labels","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fspd_f = pd.read_stata(\"/kaggle/input/fspdata/FSPD.dta\", index_col=\"id\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fspd = clean_fspd(fspd_f)\nencfspd = encode_fspd(fspd)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import DistilBertTokenizer, DistilBertModel \n\n# Initialize DistilBERT model and tokenizer\npretrained_model_name = \"distilbert-base-uncased\"\ntokenizer = DistilBertTokenizer.from_pretrained(pretrained_model_name)\nbert_model = DistilBertModel.from_pretrained(pretrained_model_name)\n\nbert_model = bert_model.to(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encfspd[\"policydecision_details_tokens\"] = encfspd[\"policydecision_details\"].apply(lambda x: tokenizer.encode(x, truncation=True, max_length=128))\nencfspd[\"policy_description_tokens\"] = encfspd[\"policy_description\"].apply(lambda x: tokenizer.encode(x, truncation=True, max_length=128))\nencfspd[\"contextoradditionalinfo_tokens\"] = encfspd[\"contextoradditionalinfo\"].apply(lambda x: tokenizer.encode(x, truncation=True, max_length=96))\nencfspd[\"source1name_tokens\"] = encfspd[\"source1name\"].apply(lambda x: tokenizer.encode(x, truncation=True, max_length=8))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_length = max(encfspd[[\"policydecision_details_tokens\", \"policy_description_tokens\", \"contextoradditionalinfo_tokens\", \"source1name\"]].applymap(len).max())\n\nencfspd[\"policydecision_details_tokens\"] = encfspd[\"policydecision_details_tokens\"].apply(lambda x: x + [0] * (max_length - len(x)))\nencfspd[\"policy_description_tokens\"] = encfspd[\"policy_description_tokens\"].apply(lambda x: x + [0] * (max_length - len(x)))\nencfspd[\"contextoradditionalinfo_tokens\"] = encfspd[\"contextoradditionalinfo_tokens\"].apply(lambda x: x + [0] * (max_length - len(x)))\nencfspd[\"source1name_tokens\"] = encfspd[\"source1name_tokens\"].apply(lambda x: x + [0] * (max_length - len(x)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encfspd[\"policydecision_details_tokens\"] = encfspd[\"policydecision_details_tokens\"].apply(lambda x: torch.tensor(x))\nencfspd[\"policy_description_tokens\"] = encfspd[\"policy_description_tokens\"].apply(lambda x: torch.tensor(x))\nencfspd[\"contextoradditionalinfo_tokens\"] = encfspd[\"contextoradditionalinfo_tokens\"].apply(lambda x: torch.tensor(x))\nencfspd[\"source1name_tokens\"] = encfspd[\"source1name_tokens\"].apply(lambda x: torch.tensor(x))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create LabelEncoder instances for lever and segment\n#lever_encoder = LabelEncoder()\nsegment_encoder = LabelEncoder()\n\n# Fit the encoders on the respective target labels and transform them\n#encfspd[\"lever\"] = lever_encoder.fit_transform(encfspd[\"lever\"])\nencfspd[\"segment\"] = segment_encoder.fit_transform(encfspd[\"segment\"])\n\n# Extract lever and segment labels from the encfspd DataFrame\n#lever_labels = encfspd[\"lever\"].values\nsegment_labels = encfspd[\"segment\"].values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create non-text-feature dataframe. It contains column 6 and then from 8 to the end\n\nslice1 = encfspd.iloc[:, 3]\nslice2 = encfspd.iloc[:, 6:]\nslice3 = encfspd.iloc[:, 10:411]\n\nnon_text_features = pd.concat([slice1, slice2, slice3], axis=1).reset_index(drop=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(non_text_features.select_dtypes(include=['object']).columns)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"non_text_features = non_text_features.drop(non_text_features.select_dtypes(include=['object']).columns, axis=1)\nprint(non_text_features.select_dtypes(include=['object']).columns)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Careful: no test!\n# train_data, val_data, train_segment, val_segment = train_test_split(encfspd, segment_labels, test_size=0.2, random_state=42)\n\n# With test\ntrain_data, temp_data, train_segment, temp_segment = train_test_split(encfspd, segment_labels, test_size=0.3, random_state=42)\nval_data, test_data, val_segment, test_segment = train_test_split(temp_data, temp_segment, test_size=0.5, random_state=42)\n\n# Compute class weights\nclass_weights = compute_class_weights(train_segment)\n\n# Compute sample weights\ntrain_sample_weights = get_sample_weights(train_segment, class_weights)\n\ntrain_dataset = FSPData(train_data, train_segment)\nval_dataset = FSPData(val_data, val_segment)\ntest_dataset = FSPData(test_data, test_segment)\n\n## Final: train on whole dataset\n# train_dataset = FSPData(encfspd, segment_labels)\n\nbatch_size = 16\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n#train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=weighted_sampler)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\ntrain_features, train_labels = extract_features(train_loader)\nval_features, val_labels = extract_features(val_loader)\ntest_features, test_labels = extract_features(test_loader)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data_lgb = lgb.Dataset(train_features, label=train_labels, weight=train_sample_weights)\nval_data_lgb = lgb.Dataset(val_features, label=val_labels)\n\nlgb_params = {\n    \"objective\": \"multiclass\",\n    \"num_class\": len(np.unique(segment_labels)),\n    \"metric\": \"multi_logloss\",\n    \"max_depth\": 6,\n    \"lambda_l1\": 0.05,\n    \"lambda_l2\": 0.05,\n    \"min_data_in_leaf\": 5,\n    \"early_stopping_round\":10,\n    \"boosting_type\": \"gbdt\",\n    \"num_leaves\": 15,\n    \"learning_rate\": 0.05,\n    \"feature_fraction\": 0.9,\n    \"bagging_fraction\": 0.8,\n    \"bagging_freq\": 5,\n    \"verbose\": 0,\n    \"num_threads\": -1,\n    \"seed\": 42,\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from sklearn.model_selection import GridSearchCV\n\n# # Define the initial parameters\n# lgb_params = {\n#     \"boosting_type\": \"gbdt\",\n#     \"objective\": \"multiclass\",\n#     \"num_class\": len(np.unique(segment_labels)),\n#     \"metric\": \"multi_logloss\",\n#     \"verbose\": 0,\n#     \"num_threads\": -1,\n#     \"seed\": 42,\n# }\n\n# # Create a LightGBM classifier\n# clf = lgb.LGBMClassifier(**lgb_params)\n\n# # Specify the parameters to search\n# param_grid = {\n#     \"num_leaves\": [31, 40, 50],\n#     \"learning_rate\": [0.01, 0.05, 0.1],\n#     \"feature_fraction\": [0.8, 0.9, 1.0],\n#     \"bagging_fraction\": [0.7, 0.8, 0.9],\n#     \"bagging_freq\": [3, 5, 7],\n# }\n\n# # Create the grid search object\n# grid = GridSearchCV(\n#     estimator=clf,\n#     param_grid=param_grid,\n#     scoring='neg_log_loss',\n#     cv=3,  # number of cross-validation folds\n#     verbose=1,\n#     n_jobs=-1,  # use all available cores for parallel processing\n# )\n\n# # Perform the grid search using the train features and labels\n# grid.fit(train_features, train_labels)\n\n# # Print the best parameters found by the grid search\n# print(\"Best parameters found by grid search:\", grid.best_params_)\n\n# # Get the best model found by the grid search\n# best_model = grid.best_estimator_\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lgb_model = lgb.train(lgb_params, train_data_lgb, num_boost_round=50, valid_sets=[train_data_lgb, val_data_lgb], verbose_eval=-100)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_preds = lgb_model.predict(val_features)\nval_preds = np.argmax(val_preds, axis=1)\nval_accuracy = np.sum(val_preds == val_labels) / len(val_labels)\n\ntest_preds = lgb_model.predict(test_features)\ntest_preds = np.argmax(test_preds, axis=1)\ntest_accuracy = np.sum(test_preds == test_labels) / len(test_labels)\n\nprint(\"Validation accuracy:\", val_accuracy)\nprint(\"Test accuracy:\", test_accuracy)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lgb_model.save_model('lightgbm_model.txt')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}